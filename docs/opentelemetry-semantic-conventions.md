# OpenTelemetry AI Semantic Conventions ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” OpenTelemetry AI Semantic Conventionsë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì‹œë³´ë“œë¥¼ êµ¬ì„±í•˜ê³ , ì´ í”„ë¡œì íŠ¸ì˜ ëŒ€ì‹œë³´ë“œê°€ ì–´ë–»ê²Œ ì„¤ê³„ë˜ì—ˆëŠ”ì§€ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“š ëª©ì°¨

1. [OpenTelemetry GenAI Semantic Conventions ê°œìš”](#opentelemetry-genai-semantic-conventions-ê°œìš”)
2. [í•µì‹¬ Span Attributes](#í•µì‹¬-span-attributes)
3. [Metrics ëª…ì„¸](#metrics-ëª…ì„¸)
4. [Events ëª…ì„¸](#events-ëª…ì„¸)
5. [ëŒ€ì‹œë³´ë“œ êµ¬í˜„ ë¶„ì„](#ëŒ€ì‹œë³´ë“œ-êµ¬í˜„-ë¶„ì„)
6. [ëŒ€ì‹œë³´ë“œ ìƒì„± ê°€ì´ë“œ](#ëŒ€ì‹œë³´ë“œ-ìƒì„±-ê°€ì´ë“œ)

---

## OpenTelemetry GenAI Semantic Conventions ê°œìš”

OpenTelemetryëŠ” GenAI(Generative AI) ì‹œìŠ¤í…œì˜ ê´€ì¸¡ì„±(Observability)ì„ í‘œì¤€í™”í•˜ê¸° ìœ„í•œ ì‹œë§¨í‹± ì»¨ë²¤ì…˜ì„ ì •ì˜í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‹¤ì–‘í•œ LLM ì œê³µì, ëª¨ë¸, í”Œë«í¼ ê°„ì˜ ì¼ê´€ëœ í…”ë ˆë©”íŠ¸ë¦¬ ë°ì´í„° ìˆ˜ì§‘ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### ì°¸ì¡° ë§í¬
- [OpenTelemetry GenAI Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/gen-ai/)
- [GitHub: Semantic Conventions](https://github.com/open-telemetry/semantic-conventions/blob/main/docs/gen-ai/README.md)

---

## í•µì‹¬ Span Attributes

### ê¸°ë³¸ ì‹ë³„ì Attributes

| Attribute | Type | í•„ìˆ˜ | ì„¤ëª… | ì˜ˆì‹œ |
|-----------|------|------|------|------|
| `gen_ai.system` | string | âœ… | GenAI ì œê³µì/ì‹œìŠ¤í…œ | `"openai"`, `"azure_openai"`, `"anthropic"` |
| `gen_ai.operation.name` | string | âœ… | ì‘ì—… ìœ í˜• | `"chat"`, `"text_completion"`, `"embeddings"` |
| `gen_ai.request.model` | string | âœ… | ìš”ì²­ëœ ëª¨ë¸ëª… | `"gpt-4"`, `"gpt-4-turbo"`, `"claude-3-opus"` |
| `gen_ai.response.model` | string | ê¶Œì¥ | ì‹¤ì œ ì‘ë‹µí•œ ëª¨ë¸ëª… | `"gpt-4-0613"` |
| `gen_ai.response.id` | string | ê¶Œì¥ | ì‘ë‹µ ê³ ìœ  ì‹ë³„ì | `"chatcmpl-abc123"` |

### ìš”ì²­ íŒŒë¼ë¯¸í„° Attributes

| Attribute | Type | ì„¤ëª… | ì˜ˆì‹œ |
|-----------|------|------|------|
| `gen_ai.request.max_tokens` | int | ìµœëŒ€ ìƒì„± í† í° ìˆ˜ | `1000` |
| `gen_ai.request.temperature` | float | ì°½ì˜ì„± ì¡°ì ˆ ì˜¨ë„ | `0.7` |
| `gen_ai.request.top_p` | float | Nucleus sampling íŒŒë¼ë¯¸í„° | `0.9` |
| `gen_ai.request.stop_sequences` | string[] | ìƒì„± ì¤‘ë‹¨ ì‹œí€€ìŠ¤ | `["\n\n"]` |
| `gen_ai.request.presence_penalty` | float | ë°˜ë³µ ë°©ì§€ íŒ¨ë„í‹° | `0.0` |
| `gen_ai.request.frequency_penalty` | float | ë¹ˆë„ íŒ¨ë„í‹° | `0.0` |

### í† í° ì‚¬ìš©ëŸ‰ Attributes

| Attribute | Type | í•„ìˆ˜ | ì„¤ëª… | ì˜ˆì‹œ |
|-----------|------|------|------|------|
| `gen_ai.usage.input_tokens` | int | âœ… | ì…ë ¥ í† í° ìˆ˜ | `120` |
| `gen_ai.usage.output_tokens` | int | âœ… | ì¶œë ¥ í† í° ìˆ˜ | `300` |
| `gen_ai.token.type` | string | âœ… (ë©”íŠ¸ë¦­ìš©) | í† í° ìœ í˜• | `"input"`, `"output"` |

### ì‘ë‹µ Attributes

| Attribute | Type | ì„¤ëª… | ì˜ˆì‹œ |
|-----------|------|------|------|
| `gen_ai.response.finish_reasons` | string[] | ìƒì„± ì¢…ë£Œ ì´ìœ  | `["stop"]`, `["length"]`, `["tool_calls"]` |

### Span ëª…ëª… ê·œì¹™

```
{gen_ai.operation.name} {gen_ai.request.model}
```

**ì˜ˆì‹œ:**
- `chat gpt-4`
- `text_completion claude-3-opus`
- `embeddings text-embedding-ada-002`

### Span Kind

- **CLIENT**: ì™¸ë¶€ LLM API í˜¸ì¶œ ì‹œ
- **INTERNAL**: í”„ë¡œì„¸ìŠ¤ ë‚´ë¶€ì—ì„œ LLM ì‹¤í–‰ ì‹œ

---

## Metrics ëª…ì„¸

### í´ë¼ì´ì–¸íŠ¸ ë©”íŠ¸ë¦­

#### `gen_ai.client.token.usage` (Histogram)

í† í° ì‚¬ìš©ëŸ‰ì„ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ì¸¡ì •í•©ë‹ˆë‹¤.

| ì†ì„± | Type | í•„ìˆ˜ | ì„¤ëª… |
|------|------|------|------|
| `gen_ai.operation.name` | string | âœ… | ì‘ì—… ìœ í˜• |
| `gen_ai.request.model` | string | âœ… | ìš”ì²­ëœ ëª¨ë¸ |
| `gen_ai.system` | string | âœ… | AI ì‹œìŠ¤í…œ |
| `gen_ai.token.type` | string | âœ… | `"input"` ë˜ëŠ” `"output"` |
| `gen_ai.response.model` | string | ê¶Œì¥ | ì‘ë‹µ ëª¨ë¸ |

**ê¶Œì¥ ë²„í‚· ê²½ê³„:**
```
[1, 4, 16, 64, 256, 1024, 4096, 16384, 65536, 262144, 1048576, 4194304, 16777216, 67108864]
```

#### `gen_ai.client.operation.duration` (Histogram)

ì‘ì—… ì§€ì—°ì‹œê°„ì„ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ì¸¡ì •í•©ë‹ˆë‹¤.

| ì†ì„± | Type | í•„ìˆ˜ | ì„¤ëª… |
|------|------|------|------|
| `gen_ai.operation.name` | string | âœ… | ì‘ì—… ìœ í˜• |
| `gen_ai.request.model` | string | âœ… | ìš”ì²­ëœ ëª¨ë¸ |
| `gen_ai.system` | string | âœ… | AI ì‹œìŠ¤í…œ |

### ì„œë²„ ë©”íŠ¸ë¦­ (LLM ì„œë¹™ìš©)

| ë©”íŠ¸ë¦­ | Type | ì„¤ëª… |
|--------|------|------|
| `gen_ai.server.request.duration` | Histogram | ì„œë²„ ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ |
| `gen_ai.server.time_per_output_token` | Histogram | ì¶œë ¥ í† í°ë‹¹ ì²˜ë¦¬ ì‹œê°„ |
| `gen_ai.server.time_to_first_token` | Histogram | ì²« í† í°ê¹Œì§€ì˜ ì‹œê°„ |

---

## Events ëª…ì„¸

GenAI ì´ë²¤íŠ¸ëŠ” LLM ìƒí˜¸ì‘ìš©ì˜ ì…ë ¥, ì¶œë ¥, ìƒíƒœë¥¼ ìº¡ì²˜í•©ë‹ˆë‹¤.

### ë©”ì‹œì§€ ì´ë²¤íŠ¸

| Event | ì„¤ëª… | ì£¼ìš” ì†ì„± |
|-------|------|----------|
| `gen_ai.system.message` | ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ | `gen_ai.system.message.content` |
| `gen_ai.user.message` | ì‚¬ìš©ì ì…ë ¥ | `gen_ai.user.message.content` |
| `gen_ai.assistant.message` | AI ì‘ë‹µ | `gen_ai.assistant.message.content` |
| `gen_ai.tool.message` | ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ | `gen_ai.tool.message.content` |

### ì„ íƒ ì´ë²¤íŠ¸

| Event | ì„¤ëª… |
|-------|------|
| `gen_ai.choice` | AIì˜ ì‘ë‹µ ì„ íƒ (ì´ìœ , ë©”íƒ€ë°ì´í„° í¬í•¨) |

---

## ëŒ€ì‹œë³´ë“œ êµ¬í˜„ ë¶„ì„

### í˜„ì¬ í”„ë¡œì íŠ¸ì˜ ëŒ€ì‹œë³´ë“œ

ì´ í”„ë¡œì íŠ¸ëŠ” ë‘ ê°œì˜ Grafana ëŒ€ì‹œë³´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤:

1. **LangGraph ìš´ì˜ ëŒ€ì‹œë³´ë“œ** (`k8s/azure-grafana-langgraph.json`)
2. **Agent Lightning í•™ìŠµ ëŒ€ì‹œë³´ë“œ** (`k8s/azure-grafana-agentlightning.json`)

### OpenTelemetry Semantic Conventions ì ìš© í˜„í™©

#### âœ… ì ìš©ëœ Conventions

| Convention | ëŒ€ì‹œë³´ë“œ ì‚¬ìš© | ì¿¼ë¦¬ ì˜ˆì‹œ |
|------------|--------------|----------|
| `gen_ai.usage.input_tokens` | LLM Metrics íŒ¨ë„ | `customDimensions['gen_ai.usage.input_tokens']` |
| `gen_ai.usage.output_tokens` | Token Usage íŒ¨ë„ | `customDimensions['gen_ai.usage.output_tokens']` |
| `gen_ai.request.model` | LLM Model Performance | `customDimensions['gen_ai.request.model']` |

#### í•˜ìœ„ í˜¸í™˜ì„±

ëŒ€ì‹œë³´ë“œëŠ” ë‹¤ì–‘í•œ ê³„ì¸¡ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ `coalesce()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

```kusto
// OpenTelemetry í‘œì¤€ê³¼ LangChain ê³„ì¸¡ ëª¨ë‘ ì§€ì›
Input = sum(toint(coalesce(
    customDimensions['gen_ai.usage.input_tokens'],      -- OTel GenAI í‘œì¤€
    customDimensions['llm.usage.prompt_tokens'],        -- ì´ì „ LangChain ê³„ì¸¡
    "0"
)))
```

### LangGraph ëŒ€ì‹œë³´ë“œ íŒ¨ë„ ë¶„ì„

| íŒ¨ë„ | ì‚¬ìš© Attributes | GenAI Convention ì í•©ì„± |
|------|----------------|------------------------|
| Overview | `operation_Id`, `duration`, `success` | âœ… ì¼ë°˜ OTel |
| LLM Summary | `gen_ai.usage.input_tokens`, `gen_ai.usage.output_tokens` | âœ… GenAI í‘œì¤€ |
| Token Usage Over Time | `gen_ai.usage.*` | âœ… GenAI í‘œì¤€ |
| LLM Model Performance | `gen_ai.request.model` | âœ… GenAI í‘œì¤€ |
| Trace Detail | `operation_Id` | âœ… ì¼ë°˜ OTel |

### Agent Lightning ëŒ€ì‹œë³´ë“œ íŒ¨ë„ ë¶„ì„

| íŒ¨ë„ | ì‚¬ìš© Attributes | ì„¤ëª… |
|------|----------------|------|
| Training Overview | `agentlightning.reward.*`, `agentlightning.rollout_id` | ì»¤ìŠ¤í…€ í•™ìŠµ ë©”íŠ¸ë¦­ |
| Reward Trend | `agentlightning.reward.0.value` | APO ìµœì í™” ë³´ìƒ |
| Rollout Summary | `agentlightning.rollout_id`, `agentlightning.attempt_id` | í•™ìŠµ ì§„í–‰ ì¶”ì  |

---

## ëŒ€ì‹œë³´ë“œ ìƒì„± ê°€ì´ë“œ

### 1. ê¸°ë³¸ LLM ëª¨ë‹ˆí„°ë§ íŒ¨ë„

#### ì´ í† í° ì‚¬ìš©ëŸ‰

```kusto
dependencies
| where name has_any ("ChatOpenAI", "AzureChatOpenAI", "openai", "llm", "gen_ai", "chat")
| summarize
    ['LLM Calls'] = count(),
    ['Input Tokens'] = sum(toint(coalesce(
        customDimensions['gen_ai.usage.input_tokens'],
        customDimensions['llm.usage.prompt_tokens'],
        "0"
    ))),
    ['Output Tokens'] = sum(toint(coalesce(
        customDimensions['gen_ai.usage.output_tokens'],
        customDimensions['llm.usage.completion_tokens'],
        "0"
    ))),
    ['Avg LLM Latency (s)'] = round(avg(duration) / 1000, 2)
```

#### ì‹œê°„ë³„ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì´

```kusto
dependencies
| where name has_any ("ChatOpenAI", "AzureChatOpenAI", "openai", "llm", "gen_ai", "chat")
| summarize 
    Input = sum(toint(coalesce(
        customDimensions['gen_ai.usage.input_tokens'],
        customDimensions['llm.usage.prompt_tokens'],
        "0"
    ))),
    Output = sum(toint(coalesce(
        customDimensions['gen_ai.usage.output_tokens'],
        customDimensions['llm.usage.completion_tokens'],
        "0"
    )))
    by bin(timestamp, 1m)
```

#### ëª¨ë¸ë³„ ì„±ëŠ¥ ë¶„ì„

```kusto
dependencies
| where name has_any ("ChatOpenAI", "AzureChatOpenAI", "openai", "llm", "gen_ai", "chat")
| extend model = coalesce(
    tostring(customDimensions['gen_ai.request.model']),
    tostring(customDimensions['llm.request.model']),
    tostring(customDimensions['model']),
    "unknown"
)
| summarize 
    Calls = count(),
    ['Avg (s)'] = round(avg(duration) / 1000, 2),
    ['P95 (s)'] = round(percentile(duration, 95) / 1000, 2),
    ['Input Tokens'] = sum(toint(coalesce(
        customDimensions['gen_ai.usage.input_tokens'],
        customDimensions['llm.usage.prompt_tokens'],
        "0"
    ))),
    ['Output Tokens'] = sum(toint(coalesce(
        customDimensions['gen_ai.usage.output_tokens'],
        customDimensions['llm.usage.completion_tokens'],
        "0"
    )))
    by Model = model
| order by Calls desc
```

### 2. ë¹„ìš© ë¶„ì„ íŒ¨ë„ (ìƒˆë¡œ ì¶”ê°€ ê°€ëŠ¥)

í† í° ì‚¬ìš©ëŸ‰ì„ ê¸°ë°˜ìœ¼ë¡œ LLM ë¹„ìš©ì„ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```kusto
let pricing = datatable(model:string, input_per_1k:real, output_per_1k:real) [
    "gpt-4", 0.03, 0.06,
    "gpt-4-turbo", 0.01, 0.03,
    "gpt-3.5-turbo", 0.0015, 0.002
];
dependencies
| where name has_any ("ChatOpenAI", "AzureChatOpenAI", "openai")
| extend model = coalesce(
    tostring(customDimensions['gen_ai.request.model']),
    "gpt-4"
)
| extend input_tokens = toint(coalesce(
    customDimensions['gen_ai.usage.input_tokens'],
    "0"
))
| extend output_tokens = toint(coalesce(
    customDimensions['gen_ai.usage.output_tokens'],
    "0"
))
| lookup kind=leftouter pricing on model
| extend cost = (input_tokens * input_per_1k / 1000) + (output_tokens * output_per_1k / 1000)
| summarize ['Total Cost ($)'] = round(sum(cost), 4) by bin(timestamp, 1h)
```

### 3. ì—ì´ì „íŠ¸ íë¦„ ë¶„ì„ íŒ¨ë„

LangGraph ë…¸ë“œ ì‹¤í–‰ íë¦„ì„ ì‹œê°í™”í•©ë‹ˆë‹¤:

```kusto
dependencies
| where operation_Id == "<trace_id>"
| where name endswith ".task"
| order by timestamp asc
| extend seq = row_number()
| project 
    id = tostring(seq),
    title = name,
    subtitle = strcat(round(duration, 0), " ms"),
    mainstat = round(duration, 0),
    arc__success = iff(success == "True", 1.0, 0.0),
    arc__failed = iff(success == "False", 1.0, 0.0)
```

### 4. ì—ëŸ¬ ë¶„ì„ íŒ¨ë„

```kusto
exceptions
| summarize 
    Errors = count(),
    ['Unique Types'] = dcount(type)
    by bin(timestamp, 5m)
```

### 5. ëŒ€ì‹œë³´ë“œ êµ¬ì„± ê¶Œì¥ ì‚¬í•­

#### ì„¹ì…˜ êµ¬ì„±

1. **Overview (ê°œìš”)**
   - Total Traces, Spans, Success Rate
   - Avg Latency, Error Count

2. **Trends (ì¶”ì´)**
   - Request Volume Over Time
   - Latency Percentiles (P50, P95, P99)

3. **LLM Metrics (LLM ë©”íŠ¸ë¦­)**
   - Token Usage Summary
   - Token Usage Over Time
   - Model Performance Comparison

4. **Operations (ì‘ì—…)**
   - Top Operations by Count
   - Slowest Operations
   - Operation Success Rate

5. **Traces (íŠ¸ë ˆì´ìŠ¤)**
   - Recent Traces Table
   - Trace Waterfall View
   - Task Execution Flow

6. **Errors (ì—ëŸ¬)**
   - Error Trends
   - Errors by Type
   - Recent Exceptions

---

## ì¶”ê°€ ê¶Œì¥ ì‚¬í•­

### 1. ëˆ„ë½ëœ GenAI Attributes ì¶”ê°€ ê³ ë ¤

í˜„ì¬ êµ¬í˜„ì—ì„œ ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ì†ì„±:

```python
# app/main.pyì—ì„œ spanì— ì¶”ê°€ ì†ì„± ì„¤ì •
span.set_attribute("gen_ai.system", "azure_openai")
span.set_attribute("gen_ai.operation.name", "chat")
span.set_attribute("gen_ai.request.temperature", 0.7)
span.set_attribute("gen_ai.response.finish_reasons", ["stop"])
```

### 2. ë¹„ìš© ì¶”ì ì„ ìœ„í•œ ì†ì„± ì¶”ê°€

```python
# ë¹„ìš© ê³„ì‚° í›„ spanì— ê¸°ë¡
estimated_cost = calculate_cost(input_tokens, output_tokens, model)
span.set_attribute("gen_ai.usage.cost", estimated_cost)
```

### 3. í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§

```python
# í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µ í’ˆì§ˆ ì§€í‘œ
span.set_attribute("gen_ai.prompt.template.name", "teacher_question")
span.set_attribute("gen_ai.prompt.version", "v1.0")
```

---

## ê²°ë¡ 

ì´ í”„ë¡œì íŠ¸ì˜ ëŒ€ì‹œë³´ë“œëŠ” OpenTelemetry GenAI Semantic Conventionsë¥¼ ì˜ ë”°ë¥´ê³  ìˆìŠµë‹ˆë‹¤:

âœ… **ê°•ì :**
- `gen_ai.usage.input_tokens`, `gen_ai.usage.output_tokens` ì‚¬ìš©
- `gen_ai.request.model` ê¸°ë°˜ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„
- í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ `coalesce()` íŒ¨í„´ ì‚¬ìš©

ğŸ“ˆ **ê°œì„  ê°€ëŠ¥ ì˜ì—­:**
- `gen_ai.system`, `gen_ai.operation.name` ì†ì„± ëª…ì‹œì  ì„¤ì •
- `gen_ai.response.finish_reasons` ì¶”ì  ì¶”ê°€
- ë¹„ìš© ì¶”ì  ë©”íŠ¸ë¦­ êµ¬í˜„

ì´ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì—¬ ì¶”ê°€ ëŒ€ì‹œë³´ë“œ íŒ¨ë„ì„ êµ¬ì„±í•˜ê±°ë‚˜, ê¸°ì¡´ ëŒ€ì‹œë³´ë“œë¥¼ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
