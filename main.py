"""FastAPI Chat Agent with LangGraph - Teacher-Student Quiz System with OpenTelemetry"""
import asyncio
import json
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Optional, AsyncGenerator
from uuid import uuid4

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

# OpenTelemetry + Traceloop for LLM tracing
from traceloop.sdk import Traceloop
from opentelemetry import trace

from langchain_core.messages import HumanMessage

from config import AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT_NAME, OTEL_EXPORTER_OTLP_ENDPOINT
from graph import (
    create_graph, 
    QuizPhase,
)

# Global
graph = None
# ÏÑ∏ÏÖòÎ≥Ñ ÏÉÅÌÉú Ï†ÄÏû• (phase, difficulty, subject Îì±)
session_states = {}


# OpenTelemetry tracer
tracer = None

def setup_opentelemetry():
    """OpenTelemetry + Traceloop Ï¥àÍ∏∞Ìôî (LLM input/output Ï∫°Ï≤ò)"""
    global tracer
    
    import os
    # Attribute Í∏∏Ïù¥ Ï†úÌïú ÎäòÎ¶¨Í∏∞ (Í∏∞Î≥∏Í∞íÏù¥ ÏûëÏïÑÏÑú LLM Î©îÏãúÏßÄÍ∞Ä ÏûòÎ¶º)
    os.environ.setdefault("OTEL_ATTRIBUTE_VALUE_LENGTH_LIMIT", "65535")
    # Traceloop content Ï∫°Ï≤ò ÌôúÏÑ±Ìôî
    os.environ.setdefault("TRACELOOP_TRACE_CONTENT", "true")
    
    # Traceloop Ï¥àÍ∏∞Ìôî - LangChain, OpenAI Îì± ÏûêÎèô Í≥ÑÏ∏°
    # exporterÎ•º ÏßÅÏ†ë ÏÉùÏÑ±ÌïòÏó¨ OTel CollectorÎ°ú Ï†ÑÏÜ°
    from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
    
    otlp_exporter = OTLPSpanExporter(
        endpoint=OTEL_EXPORTER_OTLP_ENDPOINT,
        insecure=True,
    )
    
    Traceloop.init(
        app_name="teacher-student-quiz",
        disable_batch=False,
        exporter=otlp_exporter,
    )
    
    tracer = trace.get_tracer(__name__)
    
    print(f"‚úÖ OpenTelemetry + Traceloop initialized!")
    print(f"   OTLP Endpoint: {OTEL_EXPORTER_OTLP_ENDPOINT}")
    
    return tracer


@asynccontextmanager
async def lifespan(app: FastAPI):
    global graph, tracer
    try:
        # OpenTelemetry Ï¥àÍ∏∞Ìôî
        tracer = setup_opentelemetry()
        
        graph = create_graph()
        print("‚úÖ LangGraph Teacher-Student Quiz Agent initialized!")
        print(f"   Endpoint: {AZURE_OPENAI_ENDPOINT}")
        print(f"   Deployment: {AZURE_OPENAI_DEPLOYMENT_NAME}")
    except Exception as e:
        print(f"‚ùå Failed to initialize: {e}")
        raise e
    yield
    print("Shutting down...")


app = FastAPI(
    title="LangGraph Chat Agent",
    description="Chat agent powered by LangGraph and Azure OpenAI",
    version="1.0.0",
    lifespan=lifespan
)

# Static ÌååÏùº ÏÑúÎπô
app.mount("/static", StaticFiles(directory=Path(__file__).parent / "static"), name="static")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None  # ÏÑ∏ÏÖò ID (ÏóÜÏúºÎ©¥ ÏÉàÎ°ú ÏÉùÏÑ±)


class ChatResponse(BaseModel):
    response: str
    session_id: str  # ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Í∞Ä Îã§Ïùå ÏöîÏ≤≠Ïóê ÏÇ¨Ïö©Ìï† ÏÑ∏ÏÖò ID


@app.get("/", response_class=HTMLResponse)
async def root():
    template_path = Path(__file__).parent / "templates" / "index.html"
    return template_path.read_text(encoding="utf-8")


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    if not graph:
        raise HTTPException(status_code=503, detail="Agent not initialized")
    
    try:
        # ÏÑ∏ÏÖò ID Ï≤òÎ¶¨ (ÏóÜÏúºÎ©¥ ÏÉàÎ°ú ÏÉùÏÑ±)
        session_id = request.session_id or str(uuid4())
        
        # ÏÑ∏ÏÖò ÏÉÅÌÉú Í∞ÄÏ†∏Ïò§Í∏∞ ÎòêÎäî Ï¥àÍ∏∞Ìôî
        if session_id not in session_states:
            session_states[session_id] = {
                "phase": QuizPhase.SETUP,
                "difficulty": None,
                "subject": None,
                "round_count": 0,
            }
        
        current_state = session_states[session_id]
        user_input = request.message.strip()
        
        # LangGraph ÎÇ¥Ïû• checkpointer ÏÇ¨Ïö©
        config = {"configurable": {"thread_id": session_id}}
        
        # ÌòÑÏû¨ phaseÏóê Îî∞Î•∏ Ï≤òÎ¶¨
        phase = current_state.get("phase", QuizPhase.SETUP)
        
        # Î¶¨ÏÖã Î™ÖÎ†π Ï≤òÎ¶¨
        if any(word in user_input.lower() for word in ["ÏÉàÎ°ú", "Î¶¨ÏÖã", "reset", "Îã§Ïãú", "Ï≤òÏùå"]):
            session_states[session_id] = {
                "phase": QuizPhase.SETUP,
                "difficulty": None,
                "subject": None,
                "round_count": 0,
            }
            current_state = session_states[session_id]
            phase = QuizPhase.SETUP
        
        # Îã§Ïùå Î¨∏Ï†ú Î™ÖÎ†π Ï≤òÎ¶¨
        if phase == QuizPhase.COMPLETE and any(word in user_input.lower() for word in ["Îã§Ïùå", "Í≥ÑÏÜç", "next", "continue", "Îçî"]):
            phase = QuizPhase.QUESTIONING
            current_state["phase"] = phase
        
        # Í∑∏ÎûòÌîÑ invoke Ï§ÄÎπÑ
        invoke_state = {
            "messages": [HumanMessage(content=user_input)],
            "user_input": user_input,
            "phase": phase,
            "difficulty": current_state.get("difficulty"),
            "subject": current_state.get("subject"),
            "round_count": current_state.get("round_count", 0),
        }
        
        # Í∑∏ÎûòÌîÑ Ïã§Ìñâ
        result = graph.invoke(invoke_state, config=config)
        
        # ÏÑ∏ÏÖò ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
        session_states[session_id] = {
            "phase": result.get("phase", QuizPhase.SETUP),
            "difficulty": result.get("difficulty"),
            "subject": result.get("subject"),
            "round_count": result.get("round_count", 0),
        }
        
        # Î™®Îì† ÏÉà Î©îÏãúÏßÄ ÏàòÏßë
        all_responses = []
        for msg in result["messages"]:
            if hasattr(msg, 'content') and msg.content:
                # HumanMessageÍ∞Ä ÏïÑÎãå Í≤ÉÎßå ÏàòÏßë
                if not isinstance(msg, HumanMessage):
                    all_responses.append(msg.content)
        
        # ÎßàÏßÄÎßâ AI ÏùëÎãµ Î∞òÌôò (Ïó¨Îü¨ Í∞úÎ©¥ Ìï©Ïπ®)
        response_text = "\n\n".join(all_responses) if all_responses else "ÏùëÎãµÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§."
        
        return ChatResponse(response=response_text, session_id=session_id)
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """SSE Ïä§Ìä∏Î¶¨Î∞çÏúºÎ°ú LangGraphÎ•º ÌÜµÌïú ÏóêÏù¥Ï†ÑÌä∏ ÎåÄÌôîÎ•º ÌÜ†ÌÅ∞ Îã®ÏúÑÎ°ú Ïã§ÏãúÍ∞Ñ Ï†ÑÏÜ°"""
    if not graph:
        raise HTTPException(status_code=503, detail="Agent not initialized")
    
    async def event_generator() -> AsyncGenerator[str, None]:
        try:
            # ÏÑ∏ÏÖò ID Ï≤òÎ¶¨
            session_id = request.session_id or str(uuid4())
            
            # ÏÑ∏ÏÖò ÏÉÅÌÉú Í∞ÄÏ†∏Ïò§Í∏∞ ÎòêÎäî Ï¥àÍ∏∞Ìôî
            if session_id not in session_states:
                session_states[session_id] = {
                    "phase": QuizPhase.SETUP,
                    "difficulty": None,
                    "subject": None,
                    "round_count": 0,
                }
            
            current_state = session_states[session_id]
            user_input = request.message.strip()
            
            phase = current_state.get("phase", QuizPhase.SETUP)
            
            # ÏÑ∏ÏÖò ID Ï†ÑÏÜ°
            yield f"data: {json.dumps({'type': 'session', 'session_id': session_id})}\n\n"
            
            # Î¶¨ÏÖã Î™ÖÎ†π Ï≤òÎ¶¨
            if any(word in user_input.lower() for word in ["ÏÉàÎ°ú", "Î¶¨ÏÖã", "reset", "Îã§Ïãú", "Ï≤òÏùå"]):
                session_states[session_id] = {
                    "phase": QuizPhase.SETUP,
                    "difficulty": None,
                    "subject": None,
                    "round_count": 0,
                }
                current_state = session_states[session_id]
                phase = QuizPhase.SETUP
            
            # Îã§Ïùå Î¨∏Ï†ú Î™ÖÎ†π Ï≤òÎ¶¨
            if phase == QuizPhase.COMPLETE and any(word in user_input.lower() for word in ["Îã§Ïùå", "Í≥ÑÏÜç", "next", "continue", "Îçî"]):
                phase = QuizPhase.QUESTIONING
                current_state["phase"] = phase
            
            # LangGraph ÏÑ§Ï†ï
            config = {
                "configurable": {"thread_id": session_id},
            }
            
            # OpenTelemetry spanÏúºÎ°ú Ìä∏Î†àÏù¥Ïã± (Langfuse ÏµúÏ†ÅÌôî ÏÜçÏÑ± ÏÇ¨Ïö©)
            with tracer.start_as_current_span("chat_stream") as span:
                # Langfuse Trace-Level Attributes (Î≤îÏö©)
                span.set_attribute("langfuse.trace.name", "langgraph-session")
                span.set_attribute("langfuse.session.id", session_id)
                span.set_attribute("langfuse.trace.input", user_input)
            
                # Í∑∏ÎûòÌîÑ invoke Ï§ÄÎπÑ
                invoke_state = {
                    "messages": [HumanMessage(content=user_input)],
                    "user_input": user_input,
                    "phase": phase,
                    "difficulty": current_state.get("difficulty"),
                    "subject": current_state.get("subject"),
                    "round_count": current_state.get("round_count", 0),
                }
                
                # astreamÏúºÎ°ú LangGraph Ïã§Ìñâ (stream_mode="updates"Î°ú ÎÖ∏ÎìúÎ≥Ñ Í≤∞Í≥º Ïä§Ìä∏Î¶¨Î∞ç)
                current_node = None
                node_labels = {
                    "teacher_question": "üë®‚Äçüè´ Teacher (Î¨∏Ï†ú)",
                    "student_answer": "üßë‚Äçüéì Student",
                    "teacher_evaluate": "üë®‚Äçüè´ Teacher (ÌèâÍ∞Ä)",
                }
                
                # stream_mode="updates"Î°ú ÎÖ∏ÎìúÎ≥Ñ Í≤∞Í≥º Ïä§Ìä∏Î¶¨Î∞ç
                # Note: traceloop-sdkÍ∞Ä LLM Ìò∏Ï∂ú(gen_ai.prompt, gen_ai.completion)ÏùÑ ÏûêÎèô Í≥ÑÏ∏°
                # Ïó¨Í∏∞ÏÑúÎäî ÎÖ∏Îìú Î†àÎ≤® Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Îßå Ï∂îÍ∞Ä
                final_output = ""  # ÏµúÏ¢Ö Ï∂úÎ†• Ï∂îÏ†ÅÏö©
                async for event in graph.astream(invoke_state, config=config, stream_mode="updates"):
                    for node_name, node_output in event.items():
                        print(f"[DEBUG] node={node_name}, output_keys={node_output.keys() if isinstance(node_output, dict) else 'not dict'}")
                        
                        # Î©îÏãúÏßÄ Ï∂îÏ∂ú
                        if isinstance(node_output, dict) and "messages" in node_output:
                            for msg in node_output["messages"]:
                                if hasattr(msg, "content") and msg.content:
                                    content = msg.content
                                    final_output = content  # ÏµúÏ¢Ö Ï∂úÎ†• Ï†ÄÏû•
                                    
                                    # ÎÖ∏ÎìúÎ≥Ñ ÎùºÎ≤® ÏÑ§Ï†ï
                                    label = node_labels.get(node_name, node_name)
                                    if node_name == "teacher_question":
                                        rc = current_state.get("round_count", 0) + 1
                                        current_state["round_count"] = rc
                                        label = f"üë®‚Äçüè´ Teacher (Î¨∏Ï†ú #{rc})"
                                    
                                    # ÎÖ∏Îìú ÏãúÏûë ÏïåÎ¶º
                                    if node_name in node_labels:
                                        yield f"data: {json.dumps({'type': 'node_start', 'node': node_name, 'label': label}, ensure_ascii=False)}\n\n"
                                    
                                    # Ï†ÑÏ≤¥ Î©îÏãúÏßÄ Ï†ÑÏÜ° (ÌÉÄÏù¥Ìïë Ìö®Í≥ºÎäî ÌîÑÎ°†Ìä∏ÏóêÏÑú)
                                    yield f"data: {json.dumps({'type': 'message', 'node': node_name, 'content': content}, ensure_ascii=False)}\n\n"
                                    
                                    # ÎÖ∏Îìú Ï¢ÖÎ£å
                                    if node_name in node_labels:
                                        yield f"data: {json.dumps({'type': 'node_end', 'node': node_name})}\n\n"
                                    
                                    # Îã§Ïùå ÎÖ∏Îìú ÎåÄÍ∏∞ ÌëúÏãú
                                    if node_name == "setup" and "ÌÄ¥Ï¶à ÏÑ§Ï†ï ÏôÑÎ£å" in content:
                                        yield f"data: {json.dumps({'type': 'waiting', 'message': 'üë®‚Äçüè´ TeacherÍ∞Ä Î¨∏Ï†úÎ•º Ï§ÄÎπÑ Ï§ë...'})}\n\n"
                                    elif node_name == "teacher_question":
                                        yield f"data: {json.dumps({'type': 'waiting', 'message': 'üßë‚Äçüéì StudentÍ∞Ä ÏÉùÍ∞Å Ï§ë...'})}\n\n"
                                    elif node_name == "student_answer":
                                        yield f"data: {json.dumps({'type': 'waiting', 'message': 'üë®‚Äçüè´ TeacherÍ∞Ä ÌèâÍ∞Ä Ï§ë...'})}\n\n"
                            
                                    await asyncio.sleep(0.1)
                
                # Trace output ÏÑ§Ï†ï (ÏµúÏ¢Ö ÏùëÎãµ)
                if final_output:
                    span.set_attribute("langfuse.trace.output", final_output[:10000] if len(final_output) > 10000 else final_output)
            
            # ÏµúÏ¢Ö ÏÉÅÌÉú Í∞ÄÏ†∏Ïò§Í∏∞
            final_state = graph.get_state(config)
            if final_state and final_state.values:
                session_states[session_id] = {
                    "phase": final_state.values.get("phase", QuizPhase.SETUP),
                    "difficulty": final_state.values.get("difficulty"),
                    "subject": final_state.values.get("subject"),
                    "round_count": final_state.values.get("round_count", 0),
                }
            
            # ÏôÑÎ£å Ïù¥Î≤§Ìä∏
            yield f"data: {json.dumps({'type': 'done'})}\n\n"
            
        except Exception as e:
            print(f"Streaming Error: {e}")
            import traceback
            traceback.print_exc()
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


@app.get("/health")
async def health_check():
    return {"status": "healthy", "graph_initialized": graph is not None}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
